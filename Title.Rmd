---
title: "Predicting EPL Full-Time Result using Random Forest"
author: "Your Name"
output: github_document
---

## Introduction

In this tutorial, I will demonstrate how to use betting odds data from various companies to predict the Full-Time Result (FTR) of English Premier League (EPL) matches. The possible outcomes are:
- **H**: Home win
- **D**: Draw
- **A**: Away win

This is a simplified example where the primary objective is to showcase how to implement a classification model using numerical data. Accuracy is not the main focus here. We do not take into account team effects, time effects, year effects, or any other additional contextual factors that could improve the model's predictive performance.

**The purpose** of this tutorial is to help you understand how the code works so you can adopt it and tailor it to fit your specific needs. You can apply this method to a wide range of research problems, such as classifying narratives into different themes or tones in accounting reports.

## Step 1: Load and Install Required Libraries

```{r install-libraries, eval=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# Install necessary libraries if they aren't already installed
if (!require(dplyr)) install.packages("dplyr")
if (!require(caret)) install.packages("caret")
if (!require(randomForest)) install.packages("randomForest")
if (!require(e1071)) install.packages("e1071")  # e1071 is required by caret

# Load the libraries (make sure you have installed these packages before loading libraries)
library(dplyr)
library(caret)
library(randomForest)
```

In this first step, we ensure that the necessary libraries are installed. I recommend running this code as it will install any missing packages before proceeding with the tutorial. We select only following variables for simplicity:

**FTHG** = Full Time Home Team Goals  
**FTAG** = Full Time Away Team Goals  
**FTR** = Full Time Result (H=Home Win, D=Draw, A=Away Win) OR (H=Home Win, NH= Not a Home Win). Therefore, I create a new FTR variable based on FTHG and FTAG variables  
**B365H*** = Bet365 home win odds  
**B365D** = Bet365 draw odds  
**B365A** = Bet365 away win odds  
**IWH** = Interwetten home win odds  
**IWD** = Interwetten draw odds  
**IWA** = Interwetten away win odds  
**LBH** = Ladbrokes home win odds  
**LBD** = Ladbrokes draw odds  
**LBA** = Ladbrokes away win odds  
**WHH** = William Hill home win odds  
**WHD** = William Hill draw odds  
**WHA** = William Hill away win odds   

Therefore, essentially we are using four companies betting odds to predict outcome (FTR).

## Step 2: Prepare the Data

Here, we load the dataset and prepare it for analysis. I re-coded the Full-Time Result (FTR) to represent H (Home win), D (Draw), or A (Away win), using FTHG and GTAG variables. We also select the variables representing betting odds from various companies.


```{r load-data, include=TRUE}
# Load EPL results and betting odds data from a CSV file (I downloaded this data from kaggle.com)
epl_data <- read.csv("final_dataset_with_odds.csv") %>%
  select(FTHG, FTAG, FTR, B365H, B365D, B365A, IWH, IWD, IWA, LBH, LBD, LBA, WHH, WHD, WHA)

# Recode the Full-Time Result (FTR) based on Full-Time Home Goals (FTHG) and Full-Time Away Goals (FTAG)
epl_data <- epl_data %>%
  mutate(FTR = case_when(
    FTHG > FTAG ~ "H",  # Home win
    FTHG == FTAG ~ "D", # Draw
    FTHG < FTAG ~ "A")  # Away Win
    ) %>%
  select(-FTHG, -FTAG)

# Convert FTR to a factor variable
epl_data$FTR <- as.factor(epl_data$FTR)

# check data for any inconsistencies or missing values
summary(epl_data)

```

## Step 3: Split Data into Training and Testing Sets

In this step, we split the dataset into two parts. We use 80% of the data to train our model and 20% to test its performance. This is a common approach in machine learning to avoid overfitting.


```{r spit train/test, include=TRUE}
# Set seed for reproducibility
set.seed(1234)

# Split the data into 80% training and 20% testing
train_row_numbers <- createDataPartition(epl_data$FTR, p = 0.8, list = FALSE)
train_data <- epl_data[train_row_numbers, ]
test_data <- epl_data[-train_row_numbers, ]

```

## Step 4: Train a Model using an algorithm (we use Random Forest)

In this step, we split the dataset into two parts. We use 80% of the data to train our model and 20% to test its performance. This is a common approach in machine learning to avoid overfitting.


```{r model, include=TRUE}
# Set seed for reproducibility
set.seed(100)

# Train the model using Random Forest
model_rf <- train(FTR ~ ., data = train_data, method = 'rf')
model_rf

```

Which betting companies' odds have the best explaining power?

```{r Variable imp, include=TRUE}
# Display the variable importance (which betting companies' odds have the best explaining power?)
varimp_rf <- varImp(model_rf)
plot(varimp_rf, main = "Horserace of Betting Odds")

```

## Step 5: Make Predictions and Evaluate the Model

Now, we test the model using the test dataset and predict the Full-Time Result (FTR). I use a confusion matrix to evaluate the model's performance, which tells us how well the model classifies each outcome.

```{r predict, include=TRUE}
# Make predictions on the test set
predicted <- predict(model_rf, test_data)

# Generate a confusion matrix to validate the results
confusion_matrix <- confusionMatrix(reference = test_data$FTR, data = predicted, mode = 'everything')
confusion_matrix


```

## Step 5: Conclusion

The focus of this tutorial is to demonstrate how to implement a classification model using numerical data. The model can be further optimized by incorporating additional factors (such as team performance, year effects, etc.), but for now, the goal is to show you the process so you can apply it to your own context.

For example, in accounting research, this method can be adapted to classify the tone of narratives in financial disclosures or categorize different reporting themes. We are not aiming for high accuracy here but rather to provide a basic example for learning and experimentation.


